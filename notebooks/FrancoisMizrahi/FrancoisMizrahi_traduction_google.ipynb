{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Traduction Google trad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator, LANGUAGES\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../delphes/data/final2_clean.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = df.content.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Create the translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trad(X):\n",
    "    res = []\n",
    "    count = 0\n",
    "    lan = []\n",
    "    for i in X:\n",
    "        lan.append(translator.detect(i).lang)\n",
    "#         l = translator.detect(i).lang\n",
    "#         if l in LANGUAGES.keys(): res.append(translator.translate(i, src=l, dest='en').text)\n",
    "        count +=1\n",
    "        print(count)\n",
    "#         sleep(1)\n",
    "    return lan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trad_content = sample[:2].apply(trad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Test Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translator.translate(\"представя приоритетите на германското председателство\", dest='en').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trad = translator.detect(\"Асим Адемов: ЕС дава 50 млн. евро за еднократни помощи за земеделците с българската мярка Регламент COVID-19\")\n",
    "print(trad.lang)\n",
    "print(trad.confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(trad(sample[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = translator.translate(sample[1], dest='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for t in test:\n",
    "    print(t.text)\n",
    "    print(\"\\n###########\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:49:32.512751Z",
     "start_time": "2020-09-09T08:49:32.508633Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:49:33.306807Z",
     "start_time": "2020-09-09T08:49:33.300023Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:53.400045Z",
     "start_time": "2020-09-09T08:48:46.932852Z"
    }
   },
   "outputs": [],
   "source": [
    "src_path = '../../raw_data/vectors_english.txt'\n",
    "tgt_path = '../../raw_data/vectors_spanish.txt'\n",
    "nmax = 50000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:49:38.919101Z",
     "start_time": "2020-09-09T08:49:38.911236Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
    "    print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
    "    word2id = {v: k for k, v in src_id2word.items()}\n",
    "    word_emb = src_emb[word2id[word]]\n",
    "    scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
    "    k_best = scores.argsort()[-K:][::-1]\n",
    "    for i, idx in enumerate(k_best):\n",
    "        print('%.4f - %s' % (scores[idx], tgt_id2word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:53.894839Z",
     "start_time": "2020-09-09T08:48:53.656758Z"
    }
   },
   "outputs": [],
   "source": [
    "# printing nearest neighbors in the source space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, src_embeddings, src_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:54.168082Z",
     "start_time": "2020-09-09T08:48:54.048914Z"
    }
   },
   "outputs": [],
   "source": [
    "# printing nearest neighbors in the target space\n",
    "src_word = 'cat'\n",
    "get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:57.191826Z",
     "start_time": "2020-09-09T08:48:54.297296Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, whiten=True)  # TSNE(n_components=2, n_iter=3000, verbose=2)\n",
    "pca.fit(np.vstack([src_embeddings, tgt_embeddings]))\n",
    "print('Variance explained: %.2f' % pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:57.766886Z",
     "start_time": "2020-09-09T08:48:57.339991Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_similar_word(src_words, src_word2id, src_emb, tgt_words, tgt_word2id, tgt_emb, pca):\n",
    "\n",
    "    Y = []\n",
    "    word_labels = []\n",
    "    for sw in src_words:\n",
    "        Y.append(src_emb[src_word2id[sw]])\n",
    "        word_labels.append(sw)\n",
    "    for tw in tgt_words:\n",
    "        Y.append(tgt_emb[tgt_word2id[tw]])\n",
    "        word_labels.append(tw)\n",
    "\n",
    "    # find tsne coords for 2 dimensions\n",
    "    Y = pca.transform(Y)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "\n",
    "    # display scatter plot\n",
    "    plt.figure(figsize=(10, 8), dpi=80)\n",
    "    plt.scatter(x_coords, y_coords, marker='x')\n",
    "\n",
    "    for k, (label, x, y) in enumerate(zip(word_labels, x_coords, y_coords)):\n",
    "        color = 'blue' if k < len(src_words) else 'red'  # src words in blue / tgt words in red\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points', fontsize=19,\n",
    "                     color=color, weight='bold')\n",
    "\n",
    "    plt.xlim(x_coords.min() - 0.2, x_coords.max() + 0.2)\n",
    "    plt.ylim(y_coords.min() - 0.2, y_coords.max() + 0.2)\n",
    "    plt.title('Visualization of the multilingual word embedding space')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:48:58.147621Z",
     "start_time": "2020-09-09T08:48:57.863130Z"
    }
   },
   "outputs": [],
   "source": [
    "# get 5 random input words\n",
    "src_words = ['university', 'love', 'history', 'tennis', 'research', 'conference']\n",
    "tgt_words = ['universidad', 'amor', 'historia', u'tenis',  u'investigación', 'conferencia']\n",
    "\n",
    "# assert words in dictionaries\n",
    "for sw in src_words:\n",
    "    assert sw in src_word2id, '\"%s\" not in source dictionary' % sw\n",
    "for tw in tgt_words:\n",
    "    assert tw in tgt_word2id, '\"%s\" not in target dictionary' % sw\n",
    "\n",
    "plot_similar_word(src_words, src_word2id, src_embeddings, tgt_words, tgt_word2id, tgt_embeddings, pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Language application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:49:56.482912Z",
     "start_time": "2020-09-09T08:49:55.862694Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator, LANGUAGES\n",
    "from time import sleep\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_full = [\"bulgarian\", \"catalan\", \"croatian\", \"czech\", \"danish\", \"dutch\", \"english\", \"estonian\", \n",
    "              \"finnish\", \"french\", \"german\", \"greek\", \"hungarian\", \"italian\", \"macedonian\", \"norwegian\", \n",
    "              \"polish\", \"portuguese\", \"romanian\", \"russian\", \"slovak\", \"slovenian\", \"spanish\", \"swedish\", \n",
    "              \"ukrainian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:51:45.948820Z",
     "start_time": "2020-09-09T08:51:45.944978Z"
    }
   },
   "outputs": [],
   "source": [
    "languages = [\"english\", \"french\", \"german\", \"italian\", \"polish\", \"portuguese\", \"spanish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:52:20.732137Z",
     "start_time": "2020-09-09T08:51:48.720669Z"
    }
   },
   "outputs": [],
   "source": [
    "nmax = 100000  # maximum number of word embeddings to load\n",
    "emb_dict = {}\n",
    "for lang in languages:\n",
    "    path = f\"../../raw_data/vectors_{lang}.txt\"\n",
    "    embeddings, id2word, word2id = load_vec(path, nmax)\n",
    "    emb_dict[lang] = [embeddings, id2word, word2id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:52:41.985167Z",
     "start_time": "2020-09-09T08:52:41.978851Z"
    }
   },
   "outputs": [],
   "source": [
    "def multilang_word_vector(word, emb_dict, lang=None):\n",
    "    translator = Translator()\n",
    "    if lang == None: lang = LANGUAGES[translator.detect(word).lang]\n",
    "    lang_val = LANGUAGES.values()\n",
    "    \n",
    "#     try:\n",
    "    if lang in lang_val and word in emb_dict.get(lang)[2].keys():\n",
    "        return emb_dict[lang][0][emb_dict[lang][2][word]]\n",
    "#     except:\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "    \n",
    "    return False                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:10:24.937569Z",
     "start_time": "2020-09-09T09:10:24.931556Z"
    }
   },
   "outputs": [],
   "source": [
    "def vect_tweet(tweet):\n",
    "    translator = Translator()\n",
    "    if translator.detect(tweet).lang in LANGUAGES.keys():\n",
    "        lang = LANGUAGES[translator.detect(tweet).lang]\n",
    "        words = tweet.split(\" \")\n",
    "        res = []\n",
    "        for i in words:\n",
    "            res.append(multilang_word_vector(i, emb_dict, lang))\n",
    "        return res\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:39:01.087090Z",
     "start_time": "2020-09-08T13:39:00.916951Z"
    }
   },
   "outputs": [],
   "source": [
    "LANGUAGES[translator.detect(\"asdgvatesdrjyvxdz\").lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:37:10.602718Z",
     "start_time": "2020-09-08T13:37:10.571107Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dict[\"spanish\"][2].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:54:36.315770Z",
     "start_time": "2020-09-09T08:54:36.191240Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full = pd.read_pickle(\"../../delphes/data/extended_tweet_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T08:54:40.275296Z",
     "start_time": "2020-09-09T08:54:40.262176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Poland', 'Bulgaria', 'Italy', 'Spain', 'Finland', 'Sweden',\n",
       "       'Latvia', 'Germany', 'Greece', 'Luxembourg', 'Estonia', 'Belgium',\n",
       "       'Romania', 'France', 'Denmark', 'Lithuania', 'Netherlands',\n",
       "       'Slovakia', 'Hungary', 'Slovenia', 'Croatia', 'Portugal', 'Malta',\n",
       "       'Cyprus', 'Ireland', 'Czechia', 'Austria'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:15.876951Z",
     "start_time": "2020-09-09T09:15:15.834659Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = df_full[df_full[\"country\"]  [\"Ireland\", \"France\", \"Germany\", \"Italy\", \"Poland\", \"Portugal\", \"Spain\"]]\n",
    "\n",
    "df = df_full[df_full[\"country\"] == \"Ireland\"]\n",
    "for i in [\"Poland\", \"Spain\"]:\n",
    "    df = pd.concat([df, df_full[df_full[\"country\"] == i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:18.471731Z",
     "start_time": "2020-09-09T09:15:18.467171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ireland', 'Poland', 'Spain'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T13:25:59.009177Z",
     "start_time": "2020-09-08T13:25:58.289039Z"
    }
   },
   "outputs": [],
   "source": [
    "vect_tweet(df[\"content\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean to vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T12:56:30.887475Z",
     "start_time": "2020-09-08T12:56:30.758944Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../delphes/data/extended_tweet_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:46.010186Z",
     "start_time": "2020-09-09T09:04:46.004258Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmurl_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the URLs, the #hashtag and the @user of a column made of strings.\n",
    "    Be careful to apply it BEFORE all the other preprocessing steps (if not it wont'\n",
    "    be recognized as a URL)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df[column_name] = df[column_name].str.replace('http\\S+|www.\\S+|@\\S+|#\\S+', '', case=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:47.630761Z",
     "start_time": "2020-09-09T09:04:47.628113Z"
    }
   },
   "outputs": [],
   "source": [
    "def lower_df(df, column_name):\n",
    "    '''\n",
    "    This function lowercases a column made of strings and return the dataframe.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:49.179716Z",
     "start_time": "2020-09-09T09:04:49.176429Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmnumbers_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the digits of a column made of strings.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    def remove_numbers(text):\n",
    "        return ''.join(word for word in text if not word.isdigit())\n",
    "    df[column_name] = df[column_name].apply(remove_numbers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:50.731673Z",
     "start_time": "2020-09-09T09:04:50.727401Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmpunct_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the punctuations, all the \"rt\" and remove multiple spaces\n",
    "    of a column made of strings.\n",
    "    '''\n",
    "    punct = string.punctuation\n",
    "    df = df.copy()\n",
    "    def replace_punct(text):\n",
    "        for punctu in punct:\n",
    "            text = text.replace(punctu, ' ')\n",
    "            text = text.replace(' rt ','')\n",
    "            text = \" \".join(text.split())\n",
    "        return text\n",
    "    df[column_name] = df[column_name].apply(replace_punct)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:52.291378Z",
     "start_time": "2020-09-09T09:04:52.286819Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmstopwords_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the stopwords of a column made of strings.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    stop_words = stopwords.words('english')\n",
    "    def remove_stopwords(text):\n",
    "        for word in stop_words:\n",
    "            text = text.replace(f' {word} ', ' ')\n",
    "        return text\n",
    "    df[column_name] = df[column_name].apply(remove_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:04:53.837139Z",
     "start_time": "2020-09-09T09:04:53.834055Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmemojis_df(df):\n",
    "    '''\n",
    "    This function removes all the emojis of a column made of strings.\n",
    "    Be careful to translate in latin alphabet before applying this function :\n",
    "    it also removes cyrillic alphabet.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:27.968456Z",
     "start_time": "2020-09-09T09:15:25.544748Z"
    }
   },
   "outputs": [],
   "source": [
    "nw_df = rmurl_df(df, \"content\")\n",
    "nw_df = lower_df(nw_df, \"content\")\n",
    "nw_df = rmnumbers_df(nw_df, \"content\")\n",
    "nw_df = rmpunct_df(nw_df, \"content\")\n",
    "nw_df = rmemojis_df(nw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:13:56.509226Z",
     "start_time": "2020-09-09T09:13:56.503891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24722    the northern ireland protocol must be protecte...\n",
       "24723    as a member of the special committee on beatin...\n",
       "24724    this month in we are shining a light on childr...\n",
       "24725    yesterday i told that in europe we need to kee...\n",
       "24726    this is the letter sent to an taoiseach to be ...\n",
       "24727    the guidance on preparing for the end of the t...\n",
       "24728    many irish importers and exporters rely on the...\n",
       "24729    worried amp disappointed by the failure to eng...\n",
       "24730    the withdrawal agreement is only way to protec...\n",
       "24731    the public consultation is open until septembe...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_df.content[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:36.632549Z",
     "start_time": "2020-09-09T09:15:30.605732Z"
    }
   },
   "outputs": [],
   "source": [
    "test = nw_df.content[:10].map(vect_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:39.292523Z",
     "start_time": "2020-09-09T09:15:39.289598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28284, 7)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T09:15:43.566944Z",
     "start_time": "2020-09-09T09:15:43.560352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.714"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28284 / 10 * 6 / 60 /60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
