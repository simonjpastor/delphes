{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression des éléments indésirables de nos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mep_id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>group</th>\n",
       "      <th>nat_group</th>\n",
       "      <th>twitter</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197490</td>\n",
       "      <td>Magdalena ADAMOWICZ</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Independent</td>\n",
       "      <td>Adamowicz_Magda</td>\n",
       "      <td>[@danutahuebner Bardzo dziękuję @danutahuebner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189525</td>\n",
       "      <td>Asim ADEMOV</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Citizens for European Development of Bulgaria</td>\n",
       "      <td>AdemovAsim</td>\n",
       "      <td>[135 години единна и силна България. \\nЧестит ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124831</td>\n",
       "      <td>Isabella ADINOLFI</td>\n",
       "      <td>Italy</td>\n",
       "      <td>Non-attached Members</td>\n",
       "      <td>Movimento 5 Stelle</td>\n",
       "      <td>Isa_Adinolfi</td>\n",
       "      <td>[Sembra un film, ma purtroppo è realtà: le imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>125045</td>\n",
       "      <td>Clara AGUILERA</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Group of the Progressive Alliance of Socialist...</td>\n",
       "      <td>Partido Socialista Obrero Español</td>\n",
       "      <td>ClaraAguilera7</td>\n",
       "      <td>[Clara Aguilera: \"El criterio científico debe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>204335</td>\n",
       "      <td>Alviina ALAMETSÄ</td>\n",
       "      <td>Finland</td>\n",
       "      <td>Group of the Greens/European Free Alliance</td>\n",
       "      <td>Vihreä liitto</td>\n",
       "      <td>alviinaalametsa</td>\n",
       "      <td>[@LHurttila @MariaOhisalo Kaupungilla on järje...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mep_id                 name   country  \\\n",
       "0  197490  Magdalena ADAMOWICZ    Poland   \n",
       "1  189525          Asim ADEMOV  Bulgaria   \n",
       "2  124831    Isabella ADINOLFI     Italy   \n",
       "6  125045       Clara AGUILERA     Spain   \n",
       "7  204335     Alviina ALAMETSÄ   Finland   \n",
       "\n",
       "                                               group  \\\n",
       "0  Group of the European People's Party (Christia...   \n",
       "1  Group of the European People's Party (Christia...   \n",
       "2                               Non-attached Members   \n",
       "6  Group of the Progressive Alliance of Socialist...   \n",
       "7         Group of the Greens/European Free Alliance   \n",
       "\n",
       "                                       nat_group          twitter  \\\n",
       "0                                    Independent  Adamowicz_Magda   \n",
       "1  Citizens for European Development of Bulgaria       AdemovAsim   \n",
       "2                             Movimento 5 Stelle     Isa_Adinolfi   \n",
       "6              Partido Socialista Obrero Español   ClaraAguilera7   \n",
       "7                                  Vihreä liitto  alviinaalametsa   \n",
       "\n",
       "                                             content  \n",
       "0  [@danutahuebner Bardzo dziękuję @danutahuebner...  \n",
       "1  [135 години единна и силна България. \\nЧестит ...  \n",
       "2  [Sembra un film, ma purtroppo è realtà: le imm...  \n",
       "6  [Clara Aguilera: \"El criterio científico debe ...  \n",
       "7  [@LHurttila @MariaOhisalo Kaupungilla on järje...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture et stockage de la base de données\n",
    "tweet_df = pd.read_pickle('../../delphes/data/final4_clean.csv')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = []\n",
    "sexe = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(0, tweet_df.shape[0])\n",
    "tweet_df['index'] = y\n",
    "tweet_df.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = pd.DataFrame(columns=['mep_id', 'name', 'country', 'group', 'nat_group', 'twitter', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 54min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 0\n",
    "for j in range(tweet_df.shape[0]):\n",
    "    for i in range(len(tweet_df['content'][j])):\n",
    "        new_test_df.loc[k] = tweet_df.loc[j,'mep_id':'twitter']\n",
    "        new_test_df.loc[k, 'content'] = tweet_df['content'][j][i]\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the undesirable elements in the entire dataframe\n",
    "def rmurl_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the URLs, the #hashtag and the @user of a column made of strings.\n",
    "    Be careful to apply it BEFORE all the other preprocessing steps (if not it wont'\n",
    "    be recognized as a URL)\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df[column_name] = df[column_name].str.replace('http\\S+|www.\\S+|@\\S+|#\\S+', '', case=False)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the tweet's column\n",
    "def lower_df(df, column_name):\n",
    "    '''\n",
    "    This function lowercases a column made of strings.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df[column_name] = df[column_name].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the numbers in the tweet's column\n",
    "def rmnumbers_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the digits of a column made of strings.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    def remove_numbers(text):\n",
    "        return ''.join(word for word in text if not word.isdigit())\n",
    "    df[column_name] = df[column_name].apply(remove_numbers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the undesirable punctuations in the tweet's column\n",
    "def rmpunct_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the punctuations, all the \"rt\" and remove multiple spaces\n",
    "    of a column made of strings.\n",
    "    '''\n",
    "    punct = string.punctuation\n",
    "    df = df.copy()\n",
    "    def replace_punct(text):\n",
    "        for punctu in punct:\n",
    "            text = text.replace(punctu, ' ')\n",
    "        text = text.replace(' rt ','')\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    df[column_name] = df[column_name].apply(replace_punct)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords in the tweet's column\n",
    "def rmstopwords_df(df, column_name):\n",
    "    '''\n",
    "    This function removes all the stopwords of a column made of strings.\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    stop_words = stopwords.words('english')\n",
    "    def remove_stopwords(text):\n",
    "        for word in stop_words:\n",
    "            text = text.replace(f' {word} ', ' ')\n",
    "        return text\n",
    "    df[column_name] = df[column_name].apply(remove_stopwords)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the undesirable emojis in the entire dataframe\n",
    "def rmemojis_df(df):\n",
    "    '''\n",
    "    This function removes all the emojis of a column made of strings.\n",
    "    Be careful to translate in latin alphabet before applying this function : \n",
    "    it also removes cyrillic alphabet\n",
    "    '''\n",
    "    df = df.copy()\n",
    "    df = df.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = rmurl_df(new_test_df, 'content')\n",
    "clean_df = lower_df(clean_df, 'content')\n",
    "clean_df = rmnumbers_df(clean_df, 'content')\n",
    "clean_df = rmpunct_df(clean_df, 'content')\n",
    "clean_df = rmstopwords_df(clean_df, 'content')\n",
    "clean_df = rmemojis_df(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mep_id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>group</th>\n",
       "      <th>nat_group</th>\n",
       "      <th>twitter</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24722</th>\n",
       "      <td>124988</td>\n",
       "      <td>Deirdre CLUNE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Fine Gael Party</td>\n",
       "      <td>deirdreclunemep</td>\n",
       "      <td>the northern ireland protocol must protected l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24723</th>\n",
       "      <td>124988</td>\n",
       "      <td>Deirdre CLUNE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Fine Gael Party</td>\n",
       "      <td>deirdreclunemep</td>\n",
       "      <td>as member special committee beating cancer loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24724</th>\n",
       "      <td>124988</td>\n",
       "      <td>Deirdre CLUNE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Fine Gael Party</td>\n",
       "      <td>deirdreclunemep</td>\n",
       "      <td>this month shining light childrens cancer parl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24725</th>\n",
       "      <td>124988</td>\n",
       "      <td>Deirdre CLUNE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Fine Gael Party</td>\n",
       "      <td>deirdreclunemep</td>\n",
       "      <td>yesterday told europe need keep speed amp infr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24726</th>\n",
       "      <td>124988</td>\n",
       "      <td>Deirdre CLUNE</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>Group of the European People's Party (Christia...</td>\n",
       "      <td>Fine Gael Party</td>\n",
       "      <td>deirdreclunemep</td>\n",
       "      <td>this letter sent taoiseach clear seeking clari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mep_id           name  country  \\\n",
       "24722  124988  Deirdre CLUNE  Ireland   \n",
       "24723  124988  Deirdre CLUNE  Ireland   \n",
       "24724  124988  Deirdre CLUNE  Ireland   \n",
       "24725  124988  Deirdre CLUNE  Ireland   \n",
       "24726  124988  Deirdre CLUNE  Ireland   \n",
       "\n",
       "                                                   group        nat_group  \\\n",
       "24722  Group of the European People's Party (Christia...  Fine Gael Party   \n",
       "24723  Group of the European People's Party (Christia...  Fine Gael Party   \n",
       "24724  Group of the European People's Party (Christia...  Fine Gael Party   \n",
       "24725  Group of the European People's Party (Christia...  Fine Gael Party   \n",
       "24726  Group of the European People's Party (Christia...  Fine Gael Party   \n",
       "\n",
       "               twitter                                            content  \n",
       "24722  deirdreclunemep  the northern ireland protocol must protected l...  \n",
       "24723  deirdreclunemep  as member special committee beating cancer loo...  \n",
       "24724  deirdreclunemep  this month shining light childrens cancer parl...  \n",
       "24725  deirdreclunemep  yesterday told europe need keep speed amp infr...  \n",
       "24726  deirdreclunemep  this letter sent taoiseach clear seeking clari...  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df[clean_df['country'] == 'Ireland'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         bardzo dzikuj niezalene od wadzy wolne media d...\n",
       "1         komisja przyja arcywane projekty pilotaowe aut...\n",
       "2         komisja przyja projekty pilotaowe mojego wspau...\n",
       "3         w tym dniu w tym miejscu w tej godzinie prosz ...\n",
       "4         bg nie potrzebuje by przez nikogo broniony nie...\n",
       "                                ...                        \n",
       "137295    tedenska akcija od etrtka srede zbornik bela k...\n",
       "137296    strong amp way forward european future read eu...\n",
       "137297    a date noted worrying state danger ends hu spe...\n",
       "137298    ko vsi vedo da je nekaj noro pa se iz strahu p...\n",
       "137299    tota ekipa tak se gre iz kranja z najlepim nav...\n",
       "Name: content, Length: 137300, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbase_df = clean_df[clean_df['country'] == 'Ireland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premier test Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette fonction retourne automatiquement X_train, X_test, y_train, y_test de notre base de données twitter.\n",
    "def get_train_test_objects(df):\n",
    "    '''\n",
    "    Les étapes que cette fonction réalise sont en commentaires.\n",
    "    '''\n",
    "    # Copie de la base de données pour éviter les problèmes d'assignation abusive.\n",
    "    df = df.copy() \n",
    "    # Récupération de tous les tweets et du nom du député qui les a posté. Création de la cible y.\n",
    "    df = df[['name', 'content']]\n",
    "    y = pd.get_dummies(df['name'])\n",
    "    # Transformation des tweets en suite de mots (strings) dans une liste.\n",
    "    sentences = df['content']\n",
    "    sentences_inter = []\n",
    "    for sentence in sentences:\n",
    "        sentences_inter.append(sentence.split())\n",
    "    # Séparation des données d'entraînement et de test\n",
    "    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences_inter, y, test_size = 0.3)\n",
    "    # Vectorisation des phrases\n",
    "    word2vec = Word2Vec(sentences=sentences_train)\n",
    "    # Création des données d'entrée.\n",
    "    X_train = embedding(word2vec,sentences_train)\n",
    "    X_test = embedding(word2vec,sentences_test)\n",
    "    X_train_pad = pad_sequences(X_train, padding='post',value=-1000, dtype='float32')\n",
    "    X_test_pad = pad_sequences(X_test, padding='post',value=-1000, dtype='float32')\n",
    "    # Création des données cibles.\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "    # Sorties de la fonction\n",
    "    return X_train_pad, y_train, X_test_pad, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    y = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv.vocab.keys():\n",
    "           y.append(word2vec[word])\n",
    "    return np.array(y)\n",
    "\n",
    "def embedding(word2vec, sentences):\n",
    "    \n",
    "    y = []\n",
    "    for sentence in sentences:\n",
    "        y.append(embed_sentence(word2vec, sentence))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\psmag\\.venvs\\delphes\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = get_train_test_objects(testbase_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def init_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking(mask_value = -1000))\n",
    "    model.add(layers.LSTM(15, activation='tanh'))\n",
    "    model.add(layers.Dense(20, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1470 samples, validate on 630 samples\n",
      "Epoch 1/1000\n",
      "1470/1470 [==============================] - 4s 3ms/sample - loss: 2.3020 - acc: 0.1082 - val_loss: 2.3031 - val_acc: 0.1016\n",
      "Epoch 2/1000\n",
      "1470/1470 [==============================] - 1s 780us/sample - loss: 2.2990 - acc: 0.1136 - val_loss: 2.3014 - val_acc: 0.1095\n",
      "Epoch 3/1000\n",
      "1470/1470 [==============================] - 1s 779us/sample - loss: 2.2961 - acc: 0.1388 - val_loss: 2.2996 - val_acc: 0.1206\n",
      "Epoch 4/1000\n",
      "1470/1470 [==============================] - 1s 762us/sample - loss: 2.2920 - acc: 0.1469 - val_loss: 2.2969 - val_acc: 0.1286\n",
      "Epoch 5/1000\n",
      "1470/1470 [==============================] - 1s 825us/sample - loss: 2.2865 - acc: 0.1537 - val_loss: 2.2968 - val_acc: 0.1159\n",
      "Epoch 6/1000\n",
      "1470/1470 [==============================] - 1s 754us/sample - loss: 2.2808 - acc: 0.1605 - val_loss: 2.2901 - val_acc: 0.1381\n",
      "Epoch 7/1000\n",
      "1470/1470 [==============================] - 1s 779us/sample - loss: 2.2745 - acc: 0.1707 - val_loss: 2.2849 - val_acc: 0.1381\n",
      "Epoch 8/1000\n",
      "1470/1470 [==============================] - 1s 780us/sample - loss: 2.2694 - acc: 0.1741 - val_loss: 2.2814 - val_acc: 0.1397\n",
      "Epoch 9/1000\n",
      "1470/1470 [==============================] - 1s 785us/sample - loss: 2.2613 - acc: 0.1762 - val_loss: 2.2767 - val_acc: 0.1413\n",
      "Epoch 10/1000\n",
      "1470/1470 [==============================] - 1s 770us/sample - loss: 2.2562 - acc: 0.1769 - val_loss: 2.2712 - val_acc: 0.1429\n",
      "Epoch 11/1000\n",
      "1470/1470 [==============================] - 1s 750us/sample - loss: 2.2457 - acc: 0.1850 - val_loss: 2.2687 - val_acc: 0.1524\n",
      "Epoch 12/1000\n",
      "1470/1470 [==============================] - 1s 767us/sample - loss: 2.2421 - acc: 0.1769 - val_loss: 2.2636 - val_acc: 0.1492\n",
      "Epoch 13/1000\n",
      "1470/1470 [==============================] - 1s 840us/sample - loss: 2.2331 - acc: 0.1741 - val_loss: 2.2647 - val_acc: 0.1365\n",
      "Epoch 14/1000\n",
      "1470/1470 [==============================] - 1s 781us/sample - loss: 2.2283 - acc: 0.1891 - val_loss: 2.2608 - val_acc: 0.1397\n",
      "Epoch 15/1000\n",
      "1470/1470 [==============================] - 1s 768us/sample - loss: 2.2216 - acc: 0.1891 - val_loss: 2.2541 - val_acc: 0.1429\n",
      "Epoch 16/1000\n",
      "1470/1470 [==============================] - 1s 765us/sample - loss: 2.2151 - acc: 0.1891 - val_loss: 2.2509 - val_acc: 0.1476\n",
      "Epoch 17/1000\n",
      "1470/1470 [==============================] - 1s 755us/sample - loss: 2.2115 - acc: 0.1816 - val_loss: 2.2481 - val_acc: 0.1556\n",
      "Epoch 18/1000\n",
      "1470/1470 [==============================] - 1s 772us/sample - loss: 2.2087 - acc: 0.1898 - val_loss: 2.2451 - val_acc: 0.1619\n",
      "Epoch 19/1000\n",
      "1470/1470 [==============================] - 1s 767us/sample - loss: 2.2042 - acc: 0.1816 - val_loss: 2.2515 - val_acc: 0.1460\n",
      "Epoch 20/1000\n",
      "1470/1470 [==============================] - 1s 766us/sample - loss: 2.2031 - acc: 0.1871 - val_loss: 2.2428 - val_acc: 0.1651\n",
      "Epoch 21/1000\n",
      "1470/1470 [==============================] - 1s 762us/sample - loss: 2.1974 - acc: 0.1844 - val_loss: 2.2429 - val_acc: 0.1587\n",
      "Epoch 22/1000\n",
      "1470/1470 [==============================] - 1s 764us/sample - loss: 2.1961 - acc: 0.1884 - val_loss: 2.2402 - val_acc: 0.1635\n",
      "Epoch 23/1000\n",
      "1470/1470 [==============================] - 1s 790us/sample - loss: 2.1937 - acc: 0.1898 - val_loss: 2.2413 - val_acc: 0.1635\n",
      "Epoch 24/1000\n",
      "1470/1470 [==============================] - 1s 757us/sample - loss: 2.1907 - acc: 0.1946 - val_loss: 2.2419 - val_acc: 0.1540\n",
      "Epoch 25/1000\n",
      "1470/1470 [==============================] - 1s 754us/sample - loss: 2.1918 - acc: 0.1932 - val_loss: 2.2391 - val_acc: 0.1635\n",
      "Epoch 26/1000\n",
      "1470/1470 [==============================] - 1s 749us/sample - loss: 2.1865 - acc: 0.2000 - val_loss: 2.2457 - val_acc: 0.1651\n",
      "Epoch 27/1000\n",
      "1470/1470 [==============================] - 1s 743us/sample - loss: 2.1894 - acc: 0.1912 - val_loss: 2.2432 - val_acc: 0.1683\n",
      "Epoch 28/1000\n",
      "1470/1470 [==============================] - 1s 754us/sample - loss: 2.1852 - acc: 0.1918 - val_loss: 2.2357 - val_acc: 0.1667\n",
      "Epoch 29/1000\n",
      "1470/1470 [==============================] - 1s 755us/sample - loss: 2.1829 - acc: 0.1980 - val_loss: 2.2362 - val_acc: 0.1635\n",
      "Epoch 30/1000\n",
      "1470/1470 [==============================] - 1s 746us/sample - loss: 2.1805 - acc: 0.1959 - val_loss: 2.2356 - val_acc: 0.1556\n",
      "Epoch 31/1000\n",
      "1470/1470 [==============================] - 1s 738us/sample - loss: 2.1811 - acc: 0.1973 - val_loss: 2.2514 - val_acc: 0.1508\n",
      "Epoch 32/1000\n",
      "1470/1470 [==============================] - 1s 730us/sample - loss: 2.1815 - acc: 0.1993 - val_loss: 2.2416 - val_acc: 0.1635\n",
      "Epoch 33/1000\n",
      "1470/1470 [==============================] - 1s 743us/sample - loss: 2.1812 - acc: 0.1980 - val_loss: 2.2309 - val_acc: 0.1556\n",
      "Epoch 34/1000\n",
      "1470/1470 [==============================] - 1s 735us/sample - loss: 2.1784 - acc: 0.1980 - val_loss: 2.2302 - val_acc: 0.1635\n",
      "Epoch 35/1000\n",
      "1470/1470 [==============================] - 1s 776us/sample - loss: 2.1782 - acc: 0.2000 - val_loss: 2.2289 - val_acc: 0.1667\n",
      "Epoch 36/1000\n",
      "1470/1470 [==============================] - 1s 740us/sample - loss: 2.1747 - acc: 0.2014 - val_loss: 2.2394 - val_acc: 0.1556\n",
      "Epoch 37/1000\n",
      "1470/1470 [==============================] - 1s 733us/sample - loss: 2.1756 - acc: 0.1932 - val_loss: 2.2351 - val_acc: 0.1619\n",
      "Epoch 38/1000\n",
      "1470/1470 [==============================] - 1s 731us/sample - loss: 2.1723 - acc: 0.2102 - val_loss: 2.2340 - val_acc: 0.1667\n",
      "Epoch 39/1000\n",
      "1470/1470 [==============================] - 1s 729us/sample - loss: 2.1726 - acc: 0.1973 - val_loss: 2.2331 - val_acc: 0.1556\n",
      "Epoch 40/1000\n",
      "1470/1470 [==============================] - 1s 735us/sample - loss: 2.1701 - acc: 0.2027 - val_loss: 2.2415 - val_acc: 0.1683\n",
      "Epoch 41/1000\n",
      "1470/1470 [==============================] - 1s 737us/sample - loss: 2.1751 - acc: 0.1891 - val_loss: 2.2294 - val_acc: 0.1651\n",
      "Epoch 42/1000\n",
      "1470/1470 [==============================] - ETA: 0s - loss: 2.1662 - acc: 0.204 - 1s 736us/sample - loss: 2.1700 - acc: 0.2007 - val_loss: 2.2361 - val_acc: 0.1603\n",
      "Epoch 43/1000\n",
      "1470/1470 [==============================] - 1s 751us/sample - loss: 2.1709 - acc: 0.2007 - val_loss: 2.2308 - val_acc: 0.1683\n",
      "Epoch 44/1000\n",
      "1470/1470 [==============================] - 1s 765us/sample - loss: 2.1683 - acc: 0.2095 - val_loss: 2.2308 - val_acc: 0.1667\n",
      "Epoch 45/1000\n",
      "1470/1470 [==============================] - 1s 750us/sample - loss: 2.1704 - acc: 0.2034 - val_loss: 2.2364 - val_acc: 0.1508\n",
      "Epoch 46/1000\n",
      "1470/1470 [==============================] - 1s 762us/sample - loss: 2.1652 - acc: 0.2068 - val_loss: 2.2366 - val_acc: 0.1571\n",
      "Epoch 47/1000\n",
      "1470/1470 [==============================] - 1s 770us/sample - loss: 2.1677 - acc: 0.2014 - val_loss: 2.2234 - val_acc: 0.1635\n",
      "Epoch 48/1000\n",
      "1470/1470 [==============================] - 1s 774us/sample - loss: 2.1678 - acc: 0.2034 - val_loss: 2.2253 - val_acc: 0.1635\n",
      "Epoch 49/1000\n",
      "1470/1470 [==============================] - 1s 777us/sample - loss: 2.1665 - acc: 0.2068 - val_loss: 2.2335 - val_acc: 0.1635\n",
      "Epoch 50/1000\n",
      "1470/1470 [==============================] - 1s 794us/sample - loss: 2.1657 - acc: 0.2082 - val_loss: 2.2250 - val_acc: 0.1635\n",
      "Epoch 51/1000\n",
      "1470/1470 [==============================] - 1s 772us/sample - loss: 2.1641 - acc: 0.2007 - val_loss: 2.2366 - val_acc: 0.1540\n",
      "Epoch 52/1000\n",
      "1470/1470 [==============================] - 1s 789us/sample - loss: 2.1641 - acc: 0.2007 - val_loss: 2.2393 - val_acc: 0.1556\n",
      "Epoch 53/1000\n",
      "1470/1470 [==============================] - 1s 764us/sample - loss: 2.1657 - acc: 0.2020 - val_loss: 2.2333 - val_acc: 0.1587\n",
      "Epoch 54/1000\n",
      "1470/1470 [==============================] - 1s 772us/sample - loss: 2.1611 - acc: 0.2102 - val_loss: 2.2312 - val_acc: 0.1714\n",
      "Epoch 55/1000\n",
      "1470/1470 [==============================] - 1s 779us/sample - loss: 2.1617 - acc: 0.2007 - val_loss: 2.2280 - val_acc: 0.1619\n",
      "Epoch 56/1000\n",
      "1470/1470 [==============================] - 1s 786us/sample - loss: 2.1617 - acc: 0.2095 - val_loss: 2.2358 - val_acc: 0.1651\n",
      "Epoch 57/1000\n",
      "1470/1470 [==============================] - 1s 778us/sample - loss: 2.1622 - acc: 0.1986 - val_loss: 2.2233 - val_acc: 0.1667\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470/1470 [==============================] - 1s 830us/sample - loss: 2.1635 - acc: 0.1993 - val_loss: 2.2222 - val_acc: 0.1667\n",
      "Epoch 59/1000\n",
      "1470/1470 [==============================] - 1s 934us/sample - loss: 2.1600 - acc: 0.2054 - val_loss: 2.2353 - val_acc: 0.1540\n",
      "Epoch 60/1000\n",
      "1470/1470 [==============================] - 1s 1ms/sample - loss: 2.1561 - acc: 0.2041 - val_loss: 2.2401 - val_acc: 0.1651\n",
      "Epoch 61/1000\n",
      "1470/1470 [==============================] - 1s 915us/sample - loss: 2.1596 - acc: 0.2088 - val_loss: 2.2228 - val_acc: 0.1635\n",
      "Epoch 62/1000\n",
      "1470/1470 [==============================] - 1s 825us/sample - loss: 2.1583 - acc: 0.2136 - val_loss: 2.2287 - val_acc: 0.1683\n",
      "Epoch 63/1000\n",
      "1470/1470 [==============================] - 1s 801us/sample - loss: 2.1570 - acc: 0.2143 - val_loss: 2.2253 - val_acc: 0.1587\n",
      "Epoch 64/1000\n",
      "1470/1470 [==============================] - 1s 742us/sample - loss: 2.1578 - acc: 0.2014 - val_loss: 2.2299 - val_acc: 0.1698\n",
      "Epoch 65/1000\n",
      "1470/1470 [==============================] - 1s 731us/sample - loss: 2.1567 - acc: 0.2014 - val_loss: 2.2264 - val_acc: 0.1683\n",
      "Epoch 66/1000\n",
      "1470/1470 [==============================] - 1s 737us/sample - loss: 2.1587 - acc: 0.2082 - val_loss: 2.2242 - val_acc: 0.1603\n",
      "Epoch 67/1000\n",
      "1470/1470 [==============================] - 1s 745us/sample - loss: 2.1568 - acc: 0.1973 - val_loss: 2.2303 - val_acc: 0.1635\n",
      "Epoch 68/1000\n",
      "1470/1470 [==============================] - 1s 805us/sample - loss: 2.1530 - acc: 0.2014 - val_loss: 2.2346 - val_acc: 0.1571\n",
      "Epoch 69/1000\n",
      "1470/1470 [==============================] - 1s 831us/sample - loss: 2.1544 - acc: 0.2054 - val_loss: 2.2284 - val_acc: 0.1698\n",
      "Epoch 70/1000\n",
      "1470/1470 [==============================] - 1s 836us/sample - loss: 2.1547 - acc: 0.2061 - val_loss: 2.2288 - val_acc: 0.1635\n",
      "Epoch 71/1000\n",
      "1470/1470 [==============================] - 1s 972us/sample - loss: 2.1573 - acc: 0.2034 - val_loss: 2.2331 - val_acc: 0.1683\n",
      "Epoch 72/1000\n",
      "1470/1470 [==============================] - 1s 968us/sample - loss: 2.1540 - acc: 0.2061 - val_loss: 2.2272 - val_acc: 0.1635\n",
      "Epoch 73/1000\n",
      "1470/1470 [==============================] - 1s 916us/sample - loss: 2.1499 - acc: 0.2041 - val_loss: 2.2329 - val_acc: 0.1587\n",
      "Epoch 74/1000\n",
      "1470/1470 [==============================] - 2s 1ms/sample - loss: 2.1522 - acc: 0.2054 - val_loss: 2.2250 - val_acc: 0.1651\n",
      "Epoch 75/1000\n",
      "1470/1470 [==============================] - 1s 819us/sample - loss: 2.1480 - acc: 0.2068 - val_loss: 2.2320 - val_acc: 0.1603\n",
      "Epoch 76/1000\n",
      "1470/1470 [==============================] - 1s 808us/sample - loss: 2.1490 - acc: 0.2102 - val_loss: 2.2287 - val_acc: 0.1619\n",
      "Epoch 77/1000\n",
      "1470/1470 [==============================] - 1s 791us/sample - loss: 2.1493 - acc: 0.2054 - val_loss: 2.2331 - val_acc: 0.1667\n",
      "Epoch 78/1000\n",
      "1470/1470 [==============================] - 1s 828us/sample - loss: 2.1472 - acc: 0.2068 - val_loss: 2.2260 - val_acc: 0.1698\n",
      "Epoch 79/1000\n",
      "1470/1470 [==============================] - 1s 860us/sample - loss: 2.1496 - acc: 0.2102 - val_loss: 2.2346 - val_acc: 0.1651\n",
      "Epoch 80/1000\n",
      "1470/1470 [==============================] - ETA: 0s - loss: 2.1496 - acc: 0.208 - 1s 723us/sample - loss: 2.1455 - acc: 0.2109 - val_loss: 2.2385 - val_acc: 0.1556\n",
      "Epoch 81/1000\n",
      "1470/1470 [==============================] - 1s 766us/sample - loss: 2.1476 - acc: 0.2075 - val_loss: 2.2295 - val_acc: 0.1683\n",
      "Epoch 82/1000\n",
      "1470/1470 [==============================] - 1s 755us/sample - loss: 2.1479 - acc: 0.2020 - val_loss: 2.2345 - val_acc: 0.1667\n",
      "Epoch 83/1000\n",
      "1470/1470 [==============================] - 1s 724us/sample - loss: 2.1483 - acc: 0.2041 - val_loss: 2.2277 - val_acc: 0.1698\n",
      "Epoch 84/1000\n",
      "1470/1470 [==============================] - 1s 726us/sample - loss: 2.1451 - acc: 0.2177 - val_loss: 2.2354 - val_acc: 0.1683\n",
      "Epoch 85/1000\n",
      "1470/1470 [==============================] - 1s 724us/sample - loss: 2.1458 - acc: 0.2129 - val_loss: 2.2294 - val_acc: 0.1571\n",
      "Epoch 86/1000\n",
      "1470/1470 [==============================] - 1s 716us/sample - loss: 2.1410 - acc: 0.2095 - val_loss: 2.2278 - val_acc: 0.1698\n",
      "Epoch 87/1000\n",
      "1470/1470 [==============================] - 1s 723us/sample - loss: 2.1439 - acc: 0.2034 - val_loss: 2.2346 - val_acc: 0.1651\n",
      "Epoch 88/1000\n",
      "1470/1470 [==============================] - 2s 2ms/sample - loss: 2.1431 - acc: 0.2109 - val_loss: 2.2261 - val_acc: 0.1603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ac70da15c8>"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience = 30, monitor='val_loss', restore_best_weights=True)\n",
    "model = init_model()\n",
    "model.fit(X_train, y_train, epochs=1000, validation_split = 0.3, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/900 [==============================] - 0s 195us/sample - loss: 2.2116 - acc: 0.1789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.211647842195299, 0.17888889]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\psmag\\.venvs\\delphes\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "example = ['ireland should always belong to irish',\n",
    "          'ireland should always belong to irish']\n",
    "example_inter = []\n",
    "for ex in example:\n",
    "    example_inter.append(ex.split())\n",
    "X_example = embedding(word2vec,example_inter)\n",
    "X_example_pad = pad_sequences(X_example, padding='post',value=-1000, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06270979, 0.15590073, 0.12469088, 0.05032336, 0.07766666,\n",
       "        0.11065216, 0.15880199, 0.0959359 , 0.04862554, 0.11469299],\n",
       "       [0.06270979, 0.15590073, 0.12469088, 0.05032336, 0.07766666,\n",
       "        0.11065216, 0.15880199, 0.0959359 , 0.04862554, 0.11469299]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_example_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
